{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gentle introduction to Transformers and potential sources of bias\n",
    "\n",
    "BERT stands for Bidirectional Encoders Representations from Transformers. Hence, it is important to learn about Transformers before we can understand BERT. In this notebook, we will learn about Transformers and how BERT is built.\n",
    "\n",
    "The model contains two main components\n",
    "1. The encoder: It takes all the tokens at once and generates embeddings for each token simultaneously. This helps the model to learn more about the context, grammer, and different semantics before using it for another task in the decoder layer.\n",
    "2. The decoder: It can be used to use the encoder encodings to solve a specific task. For instance, it can be trained to translate a sentence from one language to another.\n",
    "\n",
    "\n",
    "The decoder part can be trained to solve different tasks. For instance, it can be trained to solve a classification task, a regression task, or a question answering task. BERT is basically a stack of encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input\n",
    "To input sentences or paragraphs into BERT we are required to break the sentence down into words or smaller units that BERT can understand. The problem with feeding the neural network with only words is that english contains a lot of words and there is no ground source of truth with all the english words. Hence, in order to overcome this problem BERT tokenizer breaks down the sentences into tokens made from 30,000 vocabulary. For instance, \"playing\" is broken down into \"play\" and \"##ing\". This is done to reduce the number of tokens and to make the model more efficient. \n",
    "\n",
    "Note: You cannot use any other tokenizer or word to vector library to input text into BERT. You have to use to specifically use the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with BERT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The input to BERT model is made from three parts\n",
    "1. Token embeddings - discussed above\n",
    "2. Segment embeddings - This is used to differentiate between two sentences. For instance, if we are trying to solve a question answering task, we can use this to differentiate between the question and the answer.\n",
    "3. Position embeddings - This is used to differentiate between the position of the token in the sentence. For instance, if we are trying to solve a question answering task, we can use this to differentiate between the position of the question and the answer. The reason we need position embeddings is because we input all the tokens at once and the model has no way of knowing the position of the token in the sentence. Hence, we need to explicitly tell the model the position of the token in the sentence. This also helps BERT learn from context for instance, \"A person was sitting on the bank of the river\", and \"A person robbed a bank on the river bank\". In the first sentence, the word \"bank\" is used to refer to the river bank, and in the second sentence, the word \"bank\" is used to refer to the financial instituion or a bank. Hence, the model needs to learn the context of the word \"bank\" in order to solve the task. This is where the position embeddings come into play.\n",
    "\n",
    "Currently BERT can handle 512 tokens at once. If you have more than 512 tokens to input in BERT, you can break them down into smaller chunks and concante the output of each chunk. However, in this case, you will lose the benefit of the context of the entire context. This is one of the biggest limitation of BERT. It also explains the word limit for google translate that uses a similar transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "The output of BERT is divided into two parts.\n",
    "\n",
    "1. A token denoting what is the probability of the first part of text following the second part of the text. The two text parts are seprated by the CLS token. \n",
    "2. Probability distribution over all tokens. Every token has 30,000 possibilities and we output a probability distribution over all these possible tokens. \n",
    "\n",
    "The loss is calculated by cross entropy loss between the predicted probability distribution of tokens and the actual probability distribution of tokens. The loss is then backpropagated to update the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study we will study bert base uncased model. \"Uncased\" means that the models does not differentiate between upper and lower case letters.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
