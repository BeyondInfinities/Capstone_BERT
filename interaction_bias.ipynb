{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the attributes and identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of characteristics\n",
    "# read all countries from the data/countries.json file \n",
    "import json\n",
    "import ast\n",
    "countries = json.load(open(\"data/countries.json\"))\n",
    "countries = list(countries.keys())\n",
    "\n",
    "with open(\"data/languages.txt\") as f:\n",
    "    languages = ast.literal_eval(f.readlines()[0])\n",
    "\n",
    "with open(\"data/dishes.txt\") as f:\n",
    "    dishes = f.readlines()[0].split(\",\")\n",
    "\n",
    "with open(\"data/genres.txt\") as f:\n",
    "    genres = f.readlines()[0].split(\",\")\n",
    "\n",
    "with open(\"data/religions.txt\") as f:\n",
    "    religions = f.readlines()[0].split(\",\")\n",
    "\n",
    "# strip spaces and newlines\n",
    "list_stripper = lambda x: [c.strip() for c in x]\n",
    "countries = list_stripper(countries)\n",
    "languages = list_stripper(languages)\n",
    "dishes = list_stripper(dishes)[:-1]\n",
    "genres = list_stripper(genres)[:-1]\n",
    "religions = list_stripper(religions)[:-1]\n",
    "\n",
    "# number of characteristics table\n",
    "n_char = [len(countries), len(languages), len(dishes), len(genres), len(religions)]\n",
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import subplots\n",
    "# from itertools import chain, islice\n",
    "# from string import ascii_uppercase\n",
    "# from numpy.random import choice\n",
    "# from venn import venn\n",
    "# %matplotlib inline\n",
    "\n",
    "# letters = iter(ascii_uppercase)\n",
    "\n",
    "# from venn import venn\n",
    "\n",
    "# dataset_dict = {\n",
    "#     'Countries': set(countries),\n",
    "#     'Languages': set(languages),\n",
    "#     'Food': set(dishes),\n",
    "#     'Music': set(genres),\n",
    "#     'Religion': set(religions),\n",
    "# }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_template = \"They are from {Countries}.\"\n",
    "language_template = \"They speak {Languages}.\"\n",
    "food_template = \"They eat {Food}.\"\n",
    "music_template = \"They listen to {Music}.\"\n",
    "religion_template = \"They follow the religion {Religion}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose 24 random countries from the list\n",
    "from numpy.random import choice\n",
    "np.random.seed(0)\n",
    "new_countries = choice(countries, 24, replace=False)\n",
    "new_languages = choice(languages, 18, replace=False)\n",
    "new_dishes = choice(dishes, 18, replace=False)\n",
    "new_genres = choice(genres, 4, replace=False)\n",
    "new_religions = choice(religions, 4, replace=False)\n",
    "\n",
    "sample_data = {\n",
    "    'Countries': [new_countries, country_template],\n",
    "    'Languages': [new_languages, language_template],\n",
    "    'Food': [new_dishes, food_template],\n",
    "    'Music': [new_genres, music_template],\n",
    "    'Religion': [new_religions, religion_template],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a stratified sampling of the adjectives\n",
    "# to get a representative sample of adjectives\n",
    "# for each characteristic\n",
    "# weigh the attributes by their sentiment score using nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# get the sentiment score of each attribute\n",
    "def get_sentiment_score(attribute):\n",
    "    score = sid.polarity_scores(attribute)\n",
    "    return score['compound']\n",
    "\n",
    "with open(\"data/english-adjectives.txt\") as f:\n",
    "    attributes = f.readlines()\n",
    "attributes = [a.strip() for a in attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sentiment score of each attribute\n",
    "sentiment_scores = [(a,get_sentiment_score(a)) for a in attributes if get_sentiment_score(a) != 0]\n",
    "\n",
    "# plot the sentiment scores\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# get the positive and negative attributes\n",
    "\n",
    "n = len(sentiment_scores)\n",
    "bin_number = 10 \n",
    "bin_size = n // bin_number\n",
    "\n",
    "# sort the attributes by their sentiment score\n",
    "sentiment_scores.sort(key=lambda x: x[1])\n",
    "# get all the bins\n",
    "bins = [sentiment_scores[i: i + bin_size ] for i in range(0, n-1, bin_size)]\n",
    "\n",
    "# plot the bins on the histogram\n",
    "plt.hist([s[1] for s in sentiment_scores], bins=20)\n",
    "for i, b in enumerate(bins):\n",
    "    plt.axvline(x=b[-1][1], color='r')\n",
    "plt.show()\n",
    "\n",
    "# sample 1 attribute from each bin\n",
    "# to get a representative sample of adjectives\n",
    "\n",
    "sample_adjectives = []\n",
    "np.random.seed(0)\n",
    "for b in bins:\n",
    "    sample_adjectives.append(np.random.choice([i[0] for i in b],1)[0])\n",
    "\n",
    "# plot the sentiment scores of the sampled adjectives\n",
    "plt.hist([get_sentiment_score(a) for a in sample_adjectives], bins=20)\n",
    "for i, b in enumerate(bins):\n",
    "    plt.axvline(x=b[-1][1], color='r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def probs_with_multiple_mask(words,template):\n",
    "    \"\"\"\n",
    "    Find the probability of each MASK Token being a word in the list words\n",
    "    in order. \n",
    "\n",
    "    :words: list of words that we need to find the probability \n",
    "    :template: the template that we need to fill in the words (with mask tokens)\n",
    "\n",
    "    :return: list of probabilities for each word in the list words\n",
    "    \"\"\"\n",
    "    input_idx = tok.encode(template)\n",
    "    logits = bert(torch.tensor([input_idx]))[0]\n",
    "    l = F.softmax(logits, dim=1)\n",
    "\n",
    "    # find all mask tokens\n",
    "    mask_tokens = [i for i, x in enumerate(input_idx) if x == 103]\n",
    "    probs = []\n",
    "    indices = [tok.convert_tokens_to_ids(word) for word in words]\n",
    "    for mask_token, idx in zip(mask_tokens, indices):\n",
    "        cur_l = l[0, mask_token, :]\n",
    "        prob = cur_l[idx].item()\n",
    "        probs.append(prob)\n",
    "    return probs\n",
    "\n",
    "def multiple_characteristics(char_data, template, adjectives):\n",
    "    \"\"\"\n",
    "    char_data: a dictionary with the characteristics and their values, templates\n",
    "    template: a string with the template for the interaction\n",
    "    adjectives: a list of adjectives to be used in the interaction\n",
    "    returns: \n",
    "    \"\"\"\n",
    "    # make all the combination of the characteristics data \n",
    "    from collections import OrderedDict\n",
    "    char_data = OrderedDict(char_data)\n",
    "    from itertools import product\n",
    "    char_combinations = list(product(*char_data.values()))\n",
    "    key_order = list(char_data.keys())\n",
    "    # itertools maintains the order of the elements\n",
    "    print(f\"Number of combinations: {len(char_combinations)}\")\n",
    "    print(key_order)\n",
    "\n",
    "    rows = []\n",
    "    for char_combination in tqdm(char_combinations):\n",
    "        template_base = template.replace(\"{Adjective}\", \"[MASK]\") \n",
    "        probs_base = probs_with_multiple_mask(char_combination, template_base)\n",
    "        probs_base = np.sum(np.log(probs_base))\n",
    "        for adjective in adjectives:\n",
    "            for i, key in enumerate(key_order):\n",
    "                template = template.replace(\"{\" + key + \"}\", \"[MASK]\")\n",
    "                # print(template)\n",
    "            template2 = template.replace(\"{Adjective}\", adjective) \n",
    "            probs = probs_with_multiple_mask(char_combination, template2)\n",
    "            probs = np.sum(np.log(probs))\n",
    "            row =   [adjective] + [key_order] + [list(char_combination)] + [probs] + [probs_base]\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def interaction_bias(n: int, sample_data: dict, sample_adjectives: list):\n",
    "    \"\"\"\n",
    "    n: number of characteristics in the interaction\n",
    "    sample_data: a dictionary with the characteristics and their values, templates\n",
    "    sample_adjectives: a list of adjectives to be used in the interaction\n",
    "\n",
    "    returns: \n",
    "    \"\"\"\n",
    "    # generate all possible combinations of the characteristics\n",
    "    from itertools import combinations\n",
    "    char_combinations = list(combinations(sample_data.keys(), n))\n",
    "    print(f\"Number of combinations: {len(char_combinations)}\")\n",
    "    CG_score = {}\n",
    "    modified_cg_score = {}\n",
    "    df = pd.DataFrame(columns=['Adjective', 'Characteristics', 'Values', 'Probs', 'Probs_base'])\n",
    "    for i, c in enumerate(tqdm(char_combinations)):\n",
    "        template = \" \".join([sample_data[k][1] for k in c])\n",
    "        template = template + \" They are a {Adjective}.\"\n",
    "        print(f\"Interaction {i+1}: {template}\")\n",
    "        # make the sample data\n",
    "        char_data = {k: sample_data[k][0] for k in c}\n",
    "        rows = multiple_characteristics(char_data, template, sample_adjectives)\n",
    "        # for every adjective calculate variance in probabilities\n",
    "        variance = np.array([row[3]/row[4] for row in rows]) \n",
    "        CG_score[c] = variance.var(axis=0)\n",
    "        # get all adjective sentiment scores\n",
    "        sentiment_scores = np.absolute([get_sentiment_score(row[0]) for row in rows])\n",
    "        modified_cg_score[c] = np.var(variance * sentiment_scores)\n",
    "        df = df.append(pd.DataFrame(rows, columns=['Adjective', 'Characteristics', 'Values', 'Probs', 'Probs_base']))\n",
    "\n",
    "    \n",
    "    # convert to json files\n",
    "    import json\n",
    "    with open(f\"CG_score_{n}.json\", \"w\") as f:\n",
    "        json.dump(CG_score, f)\n",
    "    with open(f\"modified_cg_score_{n}.json\", \"w\") as f:\n",
    "        json.dump(modified_cg_score, f)\n",
    "    df.to_csv(f\"CG_score_{n}.csv\", index=False)\n",
    "    return CG_score, modified_cg_score, df\n",
    "\n",
    "\n",
    "        \n",
    "# reduce the sample data to just 2 sample per characteristic\n",
    "mini_sample_data = {}\n",
    "for k,v in sample_data.items():\n",
    "    mini_sample_data[k] = (v[0][:2], v[1])\n",
    "\n",
    "CG_score, modified_cg_score, df = interaction_bias(n, mini_sample_data, sample_adjectives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ff4a94ddf0fc1c58829e0067cbc3216f8106c09a77337d8a03f9933a8736174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
