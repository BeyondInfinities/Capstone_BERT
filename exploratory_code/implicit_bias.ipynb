{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit bias in BERT\n",
    "We saw that BERT performs well on most tasks and is unbiased on certain tasks. However, we put forward another hypothesis that BERT has an implicit bias towards certain groups of people. \n",
    "\n",
    "Like the [implicit bias test](https://implicit.harvard.edu/implicit/Study?tid=-1) for humans that uses answering speed to infer implicit bias in humans. This test utilizes response time to infer that people's first intuition is to associate certain words with certain groups of people. For instance, if you are asked to name a profession and you say \"doctor\" faster than \"lawyer\", then you have an implicit bias towards doctors over lawyers. The test gives mulitple pictures of African American and European American children, pleasant words, and unpleasant words. As each items appear you are asked to make responses by swiping left or right as quickly as possible. The test then uses the response time to infer implicit bias. \n",
    "\n",
    "\n",
    "We propose a similar test for BERT that proposes that BERT has two mechanisms inside the neural network, \n",
    "1. A mechanism that creates bias. \n",
    "2. A mechanism that removes bias.\n",
    "\n",
    "Ideally BERT would have none of the above mechanisms and would be completely unbiased from the beginning. However, I propose that because biases are discovered later and retraining is used to remove the biases there is slight chance that the biases are still present in the model but just hidden by the last few layers of the model. \n",
    "\n",
    "If our hypothesis is true we can make the following two predictions.\n",
    "\n",
    "1. If we remove the last few layers of BERT, we should see the implicit bias in BERT. \n",
    "\n",
    "2. If we see the difference between biased and unbiased BERT, we can see that later neural network layers are changed more than the earlier layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Measuring implicit\n",
    "# Take the firt half of hidden states from BERT and store them for later use\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Sentence_1 = \"The person is from the United States\"\n",
    "Sentence_2 = \"The person is from the India\"\n",
    "\n",
    "sentences = [Sentence_1, Sentence_2]\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def implicit_bias(sentences):\n",
    "    \"\"\"\n",
    "    Find the difference between the first half of the hidden states \n",
    "    and the second half for different sentences. \n",
    "\n",
    "    :param: sentences array cotaining each group as an index\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_implicit_bias(implicit_bias):\n",
    "    \"\"\"\n",
    "    Plot the implicit bias for each sentence\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "n = 2 # 2, 3, 4, 6 or 12\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import embedding\n",
    "implicit_bias = defaultdict(list)\n",
    "for sentence in sentences:\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt')\n",
    "    output = model(**encoded_input, output_hidden_states=True)\n",
    "    hidden_states = output[2]\n",
    "    embedding_output = hidden_states[0]\n",
    "    hidden_states = hidden_states[1:]\n",
    "    # divide the hidden states into n pieces and store them for later use\n",
    "    n_halfs = len(hidden_states) // n\n",
    "\n",
    "    # get the first half of the hidden states\n",
    "    for i in range(n):\n",
    "        implicit_bias[sentence].append(hidden_states[i * n_halfs: (i + 1) * n_halfs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer(\"They are from United States\", return_tensors='pt')\n",
    "output = model(**encoded_input, output_hidden_states=True)\n",
    "hidden_states = output[2]\n",
    "\n",
    "# Lets print the length of the hidden states\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 8, 768])\n",
      "Embedding output torch.Size([1, 8, 768])\n",
      "Tokenized input torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# 13 \n",
    "embedding_output = hidden_states[0]\n",
    "attention_hidden_states = hidden_states[1:]\n",
    "\n",
    "for i in range(len(attention_hidden_states)):\n",
    "    print(attention_hidden_states[i].shape)\n",
    "print(\"Embedding output\", embedding_output.shape)\n",
    "print(\"Tokenized input\", encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1: there are more bias suppresors in the later layers due to debiasing training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a biased sentence and find the activation in the first half of the hidden states\n",
    "# and the second half of the hidden states\n",
    "# Compare the difference between the two halves\n",
    "\n",
    "# Input a biased sentence and find the activation in the first half of the hidden states\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ff4a94ddf0fc1c58829e0067cbc3216f8106c09a77337d8a03f9933a8736174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
