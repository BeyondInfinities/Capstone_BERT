{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we weigh attributes?\n",
    "\n",
    "Imagine we give BERT the following sentence\n",
    "\"A person from [MASK] is a doctor\" and ask it to predict the masked word. It can predict \"Germany\" with a high probability and in a implicit sense it means that it favors Germany positively over other countries. But now take another sentence \"A person from [MASK] is a engineer\" and ask it to predict the masked word. It can predict \"US\" with a high probability and in a implicit sense it means that it favors US positively over other countries. However, are these two sentences really equivalent? In the first sentence, we are talking about a doctor and in the second sentence we are talking about an engineer. Both of them are positive attributed but there is no way to weight them in order to create a general metric.  \n",
    "\n",
    "\n",
    "Basically the problem is we are given sentences with attributes and probability values. We want to find a way to weight these sentences with their probability values in order to create a general metric to measure bias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1. \n",
    "Use a sentiment analysis algorithm to make sense of the sentiment of the sentence. Sentiment analysis takes in a sentence and spits out a vector that defines different sentiments values on attributes like positiveness, openess and others. We can then use these values to weight the two sentences as they are comparable now. \n",
    "\n",
    "The advantages of this solution is that \n",
    "1. It is very simple and easy to implement. \n",
    "2. Computationally cheap and fast.\n",
    "\n",
    "The disadvantages of this solution is that\n",
    "1. It relies on another model to predict the metrics.\n",
    "2. We are not sure if this algorithm is unbiased. \n",
    "\n",
    "Hence, this method is not a good solution to measure bias because we can actually add bias to the common metric we will derive from this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_library(name):\n",
    "    try:\n",
    "        return __import__(name)\n",
    "    except ImportError:\n",
    "        import pip\n",
    "        pip.main(['install', name])\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()\n",
    "        return __import__(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small test to prove sentiment analysis could be biased \n",
    "# we will use the most popular sentiment analysis library in python: TextBlob\n",
    "\n",
    "# import the library\n",
    "import country_list\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Please install TextBlob library\")\n",
    "    !pip install textblob\n",
    "    from textblob import TextBlob\n",
    "\n",
    "# define a function to get the sentiment of a sentence\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    return TextBlob(sentence).sentiment.polarity\n",
    "\n",
    "country = \"United States\"\n",
    "\n",
    "\n",
    "import country_list\n",
    "countries = country_list.countries_for_language('en')\n",
    "\n",
    "sentiments = []\n",
    "# get the sentiment of a sentence\n",
    "for country in countries:\n",
    "    sentence = f\"People from {country[1]} are doctors\"\n",
    "    sentiments.append((country[1],get_sentiment(sentence)))\n",
    "\n",
    "# plot the all the sentimes \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist([x[1] for x in sentiments], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the countries that have the highest sentiment\n",
    "print(\"The countries that have the highest sentiment are:\")\n",
    "for sentiment in sorted(sentiments, key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we did have some outliers for the countries (namely New Caledonia, New Zealand, Papua New Guinea). Hence, we cannot use sentiment analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the word \"New\" from the country name and using the sentiment analysis again\n",
    "sentiments = []\n",
    "# get the sentiment of a sentence\n",
    "for country in countries:\n",
    "    sentence = f\"People from {country[1].replace('New ', '')} are doctors\"\n",
    "    sentiments.append((country[1],get_sentiment(sentence)))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist([x[1] for x in sentiments], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However one can also see this sentence does not have a positive sentiment. Hence, we cannot use this method. Other methods that involve sentiment analysis are harder to get by and are sometimes trained on BERT itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems with solution 1 is that we are not sure if the sentiment analysis algorithm is unbiased. Hence, we want to find a way to weight the sentences without relying on another model. One way to use another method is TFIDF. TFIDF is a method to weight the importance of a word in a sentence. It is a very common method used in NLP. However, we need to train this method based on a larger dataset because this method needs training for all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tfidf to get the most important words in a sentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([f\"People from India and {country[1]} are doctors\" for country in countries])\n",
    "# get the most important words\n",
    "print(X.shape)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the analysis just prints the country names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$bias(Germany) = log(\\frac{P_{BERT} (Germany|\"is\")}{\\sum_i P_{BERT} (country_i | \"is\")}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION 2\n",
    " \n",
    "To solve the problem of weighing different sentences and creating a score, one can use positive and negative examples. \n",
    "1. Positive examples are sentences that contain a positive attribute and have the corresponding country name as output with a high probability (say > 0.8). The score of these sentences is 1/P(country_name|sentence) where P denotes the conditional probability given by BERT's prediction for this sentence-output pair in our dataset, i.e., we want to encourage it to predict such pairs correctly but not penalize if there are other countries which also get predicted well because they might be good candidates too!  \n",
    "2 . Negative examples on the other hand can either include: negative attributes or non-corresponding outputs with respect to their input text containing an entity mentioned either way both should give us some information about bias so we will consider them all together here - letâ€™s call those entities \"incorrect\" entities; hence, our new label becomes whether something is correct vs incorrect instead of just being one particular thing like Germany, etc. We now use only two classes: correct & incorrect classifications; since having more than 2 labels would require a much larger training data size and even then may lead to overfitting due mainly because how limited amount of labeled data you could collect from Wikipedia pages! Also note that while calculating scores.\n",
    "\n",
    "Pros:\n",
    "1. Has a nice inverse curve\n",
    "2. Has a cutoff so small probabilities don't make a difference\n",
    "\n",
    "Cons:\n",
    "1. Need to classify sentences based on attributes\n",
    "2. No comparison between attributes\n",
    "3. Multiple attributes are not supported. \n",
    "\n",
    "## Solution 3\n",
    "\n",
    "Weighing different attributes using word2vec and clustering. \n",
    "1. Use word2vec to get the vector representation of each attribute and cluster them using kmeans or other clustering algorithms.  \n",
    "2. Weigh different attributes based on their distance from one another in order for us to find a metric that can compare two sentences with multiple attributes\n",
    "3.  Group biases are multiplied by an extra factor determined by the distance between a person and people distance. For instance \"People from X are Y\" and \"A person from X is a Y\". The former has higher weight than the latter. \n",
    "4. \n",
    "\n",
    "The advantages of this solution are: \n",
    "1) It is very simple and easy to implement, especially if you use word2vec which already has pretrained models available online.   2) Computationally cheap and fast as well since we don't need any extra training data besides what's needed for BERT or other NLP algorithms like LSTM 3) This method does not rely on any external model 4 ) We have control over how much weighting each attribute gets 5). The results from this algorithm were more accurate compared against our baseline 6 ). This allows comparison between multiple attributes at once 7). Can be used on many types of text such as reviews, tweets etc 8 ). Sentiment analysis could also be incorporated into it 9 ) If there was an entity mentioned in the sentence but did not get classified correctly by BERT then using clustering will allow us detect those cases 10 . Word embeddings capture syntactic information so even though words may mean different things they would still cluster together based off their context 11 . Doesnt require large amount of labeled data 12 . Not biased towards positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "# get the tfidf of the words\n",
    "tfidf = X.toarray()\n",
    "# get the most important words\n",
    "words = vectorizer.get_feature_names()\n",
    "# get the tfidf of the most important words\n",
    "tfidf = tfidf[:,np.argsort(tfidf.sum(axis=0))[-10:]]\n",
    "# get the countries\n",
    "countries = [country[1] for country in countries]\n",
    "# create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text files and import templates\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# read the text files\n",
    "def read_text_files(path):\n",
    "    files = os.listdir(path)\n",
    "    files = [file for file in files if file.endswith(\".txt\")]\n",
    "    files = [os.path.join(path, file) for file in files]\n",
    "    return files\n",
    "\n",
    "# read the text files\n",
    "path = \"templates\"\n",
    "files = read_text_files(path)\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the sentences in the text files\n",
    "def get_sentences(file):\n",
    "    sentences = []\n",
    "    with open(file, \"r\") as f:\n",
    "        sentences.extend(f.readlines())\n",
    "        # remove the new line character\n",
    "        sentences = [sentence.replace(\"\\n\", \"\") for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "# get all the sentences in the text files\n",
    "sentence_english = get_sentences(files[7])\n",
    "words_english = get_sentences(files[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the vector differences between the words_english with two clusters\n",
    "# centered around the words \"good\" and \"bad\"\n",
    "\n",
    "# import words2vec\n",
    "import_library(\"gensim\")\n",
    "from gensim.models import KeyedVectors\n",
    "# load the model that contains the word vectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the words using \n",
    "def get_vector(word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError as e:\n",
    "        try:\n",
    "            return model[word.lower()]\n",
    "        except KeyError as e:\n",
    "            try:\n",
    "                return model[word.lower().split(\" \")[0]]\n",
    "            except KeyError as e:\n",
    "                print(f\"Word {word} not found\")\n",
    "                return np.zeros(300)\n",
    "\n",
    "# make a dictionary of the words and their vectors\n",
    "words = {}\n",
    "for word in words_english:\n",
    "    words[word] = get_vector(word)\n",
    "\n",
    "# convert the dictionary to a numpy array\n",
    "vectors = np.array(list(words.values()))\n",
    "assert all([vector.shape == (300,) for vector in vectors])\n",
    "\n",
    "# get the vector of the words \"good\" and \"bad\"\n",
    "good_vector = model[\"good\"]\n",
    "bad_vector = model[\"bad\"]\n",
    "\n",
    "print(vectors.shape)\n",
    "# from vectors find the vectors that are closest to the vector of the word \"good\" and \"bad\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# get the cosine similarity between the vectors and the vector of the word \"good\"\n",
    "cosine_similarity_good = cosine_similarity(vectors, good_vector.reshape(1, -1))\n",
    "# get the cosine similarity between the vectors and the vector of the word \"bad\"\n",
    "cosine_similarity_bad = cosine_similarity(vectors, bad_vector.reshape(1, -1))\n",
    "\n",
    "# get the words that are closest to the vector of the word \"good\" and \"bad\"\n",
    "good_vectors = [vectors[i[0]] for i in np.argsort(cosine_similarity_good, axis=0)[-10:]]\n",
    "bad_vectors = [vectors[i[0]] for i in np.argsort(cosine_similarity_bad, axis=0)[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the words that are close to the words \"good\" and \"bad\" using plotly\n",
    "# interactive plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# find the words associated with each of the vectors\n",
    "def words_from_vector(vectors):\n",
    "    answer = []\n",
    "    for vector in vectors:\n",
    "        for word, vector_ in words.items():\n",
    "            if np.all(vector == vector_):\n",
    "                answer.append((word, vector))\n",
    "    return answer\n",
    "\n",
    "good_words = words_from_vector(good_vectors)\n",
    "bad_words = words_from_vector(bad_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try 4 different dimension reduction techniques\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA, FactorAnalysis\n",
    "# get the 2d vectors using PCA\n",
    "for model in [PCA, TruncatedSVD, FastICA, FactorAnalysis]:\n",
    "    base_model = model(n_components=2)\n",
    "    base_model.fit(vectors)\n",
    "    # get the 2d vectors\n",
    "    good_vectors_2d = base_model.transform([v[1] for v in good_words])\n",
    "    bad_vectors_2d = base_model.transform([v[1] for v in bad_words])\n",
    "    # plot the vectors and label them with good_words and bad_words\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=good_vectors_2d[:,0], y=good_vectors_2d[:,1], mode=\"markers\", name=\"good\"))\n",
    "    fig.add_trace(go.Scatter(x=bad_vectors_2d[:,0], y=bad_vectors_2d[:,1], mode=\"markers\", name=\"bad\"))\n",
    "    # add title\n",
    "    fig.update_layout(title_text=f\"2d vectors of the words 'good' and 'bad' using {model.__name__}\")\n",
    "    # plot good and bad as the center point of the cluster\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define function to calculate Gini coefficient\n",
    "def gini(x):\n",
    "    total = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        total += np.sum(np.abs(xi - x[i:]))\n",
    "    return total / (len(x)**2 * np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
